# Awesome Knowledge-Distillation

- [Awesome Knowledge-Distillation](#awesome-knowledge-distillation)
  - [differnet form of knowledge](#differnet-form-of-knowledge)
    - [knowledge from logits](#knowledge-from-logits)
    - [knowledge from intermediate layers](#knowledge-from-intermediate-layers)
    - [Self-KD](#self-kd)
  - [KD + GAN](#kd--gan)
  - [KD + Meta-learning](#kd--meta-learning)
  - [KD + AutoML](#kd--automl)
  - [Multi-teaacher KD](#multi-teaacher-kd)
  - [Application of KD](#application-of-kd)
  - [Beyond](#beyond)

## differnet form of knowledge

### knowledge from logits

1. Distilling the knowledge in a neural network. Hinton et al. arXiv:1503.02531
2. Learning using privileged information: similarity control and knowledge transfer. Vapnik, Vladimir and Rauf, Izmailov. MLR 2015 
3. Unifying distillation and privileged information. Lopez-Paz, David et al. arXiv 2015
4. A Gift from Knowledge Distillation: Fast Optimization, Network Minimization and Transfer Learning. Yim, Junho et al. CVPR 2017
5. Knowledge distillation by on-the-fly native ensemble. Lan, Xu et al. NIPS 2018
6. Learning Metrics from Teachers: Compact Networks for Image Embedding. Yu, Lu et al. CVPR 2019
7. Relational Knowledge Distillation.  Park, Wonpyo et al, CVPR 2019
8. Like What You Like: Knowledge Distill via Neuron Selectivity Transfer. Huang, Zehao and Wang, Naiyan. 2017
9. Correlation Congruence for Knowledge Distillation.Peng, Baoyun et al. ICCV 2019

### knowledge from intermediate layers

1. Fitnets: Hints for thin deep nets. Romero, Adriana et al. arXiv:1412.6550
2. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. Zagoruyko et al. ICLR 2017
3. Knowledge Distillation via Instance Relationship Graph. Liu, Yufan et al. CVPR 2019 
4. Knowledge Distillation via Route Constrained Optimization. Jin, Xiao et al. ICCV 2019
5. Similarity-Preserving Knowledge Distillation. Tung, Frederick, and Mori Greg. ICCV 2019
6. MEAL: Multi-Model Ensemble via Adversarial Learning. Shen,Zhiqiang, He,Zhankui, and Xue Xiangyang. AAAI 2019

### Self-KD

1. Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation. Zhang, Linfeng et al. ICCV 2019
2. Learning Lightweight Lane Detection CNNs by Self Attention Distillation. Hou, Yuenan et al. ICCV 2019

## KD + GAN

1. Training Shallow and Thin Networks for Acceleration via Knowledge Distillation with Conditional Adversarial Networks. Xu, Zheng et al. ArXiv:1709.00513
2. KDGAN:Knowledge Distillation with Generative Adversarial Networks. Wang, Xiaojie. NIPS 2018
3. [noval]DAFL:Data-Free Learning of Student Networks. Chen, Hanting et al. ICCV 2019
4. MEAL: Multi-Model Ensemble via Adversarial Learning. Shen,Zhiqiang, He,Zhankui, and Xue Xiangyang. AAAI 2019

## KD + Meta-learning

1. Zero-Shot Knowledge Distillation in Deep Networks. Nayak, Gaurav Kumar et al, AAAI 2019
2. Learning What and Where to Transfer. Jang, Yunhun et al, ICML 2019
3. Transferring Knowledge across Learning Processes. Moreno, Pablo G et al. ICLR 2019

## KD + AutoML

1. Improving Neural Architecture Search Image Classifiers via Ensemble Learning. Macko, Vladimir et al. 2019

## Multi-teaacher KD 

1. Learning from Multiple Teacher Networks. You, Shan et al. KDD 2017
2. Deep Model Compression: Distilling Knowledge from Noisy Teachers.  Sau, Bharat Bhusan et al. arXiv:1610.09650v2 
3. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Tarvainen, Antti and Valpola, Harri. NIPS 2017
4. Deep Mutual Learning. Zhang, Ying et al. CVPR 2018

## Application of KD

1. Distilled Person Re-identification: Towars a More Scalable System. Wu, Ancong et al. CVPR 2019
2. [noval]Efficient Video Classification Using Fewer Frames. Bhardwaj, Shweta et al. CVPR 2019
3. Fast Human Pose Estimation. Zhang, Feng et al. CVPR 2019
4. Distilling knowledge from a deep pose regressor network. Saputra et al. arXiv:1908.00858 (2019)
5. Patient Knowledge Distillation for BERT Model Compression. Sun, Siqi et al. arXiv:1908.09355
6. TinyBERT: Distilling BERT for Natural Language Understanding. Jiao, Xiaoqi et al. arXiv:1909.10351
7. Learning Lightweight Lane Detection CNNs by Self Attention Distillation. Hou, Yuenan et al. ICCV 2019
8. Structured Knowledge Distillation for Semantic Segmentation. Liu, Yifan et al. CVPR 2019
9. Relation Distillation Networks for Video Object Detection. Deng, Jiajun et al. ICCV 2019
10. Teacher Supervises Students How to Learn From Partially Labeled Images for Facial Landmark Detection. Dong, Xuanyi and Yang, Yi. ICCV 2019

## Beyond

1. Do deep nets really need to be deep?. Ba,Jimmy, and Rich Caruana. NIPS 2014
2. Born-Again Neural Networks. Furlanello, Tommaso et al. ICML 2018
3. When Does Label Smoothing Help? MÃ¼ller, Rafael, Kornblith, and Hinton. NIPS 2019
4. Towards Understanding Knowledge Distillation. Phuong, Mary and Lampert, Christoph. AAAI 2019

---
Note: All papers pdf can be found and downloaded on bing or Google.
